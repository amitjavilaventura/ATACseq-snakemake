# ----- Import libraries ----- #
import pandas as pd
import yaml
from snakemake.utils import validate, min_version

# ----- Minimum Snakemake version ----- #
min_version("5.4.3")

# ----- Singularity image path ----- #
singularity: "/hpcnfs/data/DP/Singularity/dfernandezperez-bioinformatics-singularity-master-chipseq.simg"

# ----- Configfile ----- #
configfile: "configuration/config.yaml"

# ----- CLUSTER FILE ----- #
# variable for the cluster file 
CLUSTER     = yaml.load(open(config['cluster'], 'r'), Loader=yaml.FullLoader)

# Determine files for samples and units
SAMPLES     = pd.read_csv(config['samples'], sep = "\t").set_index("NAME", drop=False).sort_index()
units       = pd.read_csv(config['units'], dtype=str, sep = "\t").set_index(["sample", "lane"], drop=False).sort_index()
units.index = units.index.set_levels([i.astype(str) for i in units.index.levels])  # enforce str in index

# ----- Get the samples name, input, etc ----- #
ALL_SAMPLES    = SAMPLES.NAME
ALL_CONTROLS   = SAMPLES.INPUT
SAMPLES_GENOME = SAMPLES.GENOME

ALL_CONDITIONS = set(SAMPLES.CONDITION)


# ----- Determine output files ----- #
ALL_BAMs         = expand("results/02_bam/{sample}.bam", sample = ALL_SAMPLES)

ALL_QCs          = ["results/01_QCs/multiQC/multiQC_report.html"]
ALL_FASTQCs      = expand("results/01_QCs/fastQC/{sample}_fastqc.zip", sample=ALL_SAMPLES)

ALL_BIGWIG       = expand("results/02_bigwig/{sample}.bw", sample=ALL_SAMPLES)
ALL_BW2SERVER    = expand("results/temp_file_{sample}.txt", sample = ALL_SAMPLES)

if config["options"]["peakcaller"] == "macs2":

    if config["options"]["idr_merge"] == False:
        ALL_PEAKS      = expand("results/03_macs2/{sample}/{sample}_peaks.narrowPeak", zip, sample=ALL_SAMPLES)
        ALL_PEAKS_FILT = expand("results/03_macs2/{sample}/{sample}_peaks_" + config["params"]["macs2"]["p_or_q"] + config["params"]["macs2"]["filt_peaks_pqval"] + ".bed", zip, sample=ALL_SAMPLES)
        ALL_PEAKANNOT  = expand("results/04_peakAnno/{sample}/{sample}-peaks_log" + config["params"]["macs2"]["p_or_q"] + config["params"]["macs2"]["filt_peaks_pqval"] + ".annot", zip, sample=ALL_SAMPLES)
        
    elif config["options"]["idr_merge"] == False:
        ALL_PEAKS      = expand("results/03_macs2/{sample}/{sample}_peaks.narrowPeak", zip, sample=ALL_SAMPLES)
        # ALL_PEAKANNOT  = expand("results/04_peakAnno/{sample}/{sample}-peaks_logp" + config["params"]["macs2"]["filt_peaks_pqval"] + ".annot", zip, sample=ALL_SAMPLES)
        ALL_IDR        = expand("results/03_macs2/IDR/{condition}.txt", zip, condition=ALL_CONDITIONS)
        

elif config["options"]["peakcaller"] == "genrich":

    if config["options"]["genrich_merge"] == False:
        ALL_PEAKS      = expand("results/03_genrich/{sample}/{sample}_peaks.narrowPeak", zip, sample=ALL_SAMPLES)
        ALL_PEAKS_FILT = expand("results/03_genrich/{sample}/{sample}_peaks_" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt_peaks_pqval"] + ".bed", zip, sample=ALL_SAMPLES)
        ALL_PEAKANNOT  = expand("results/04_peakAnno/{sample}/{sample}-peaks_log" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt_peaks_pqval"] + ".annot", zip, sample=ALL_SAMPLES)

    elif config["options"]["genrich_merge"] == True:
        ALL_PEAKS      = expand("results/03_genrich/merged/{condition}/{condition}_peaks.narrowPeak", zip, condition=ALL_CONDITIONS)
        ALL_PEAKS_FILT = expand("results/03_genrich/merged/{condition}/{condition}_peaks_" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt_peaks_pqval"] + ".bed", zip, condition=ALL_CONDITIONS)
        ALL_PEAKANNOT  = expand("results/04_peakAnno/merged/{condition}/{condition}-peaks_log" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt_peaks_pqval"] + ".annot", zip, condition=ALL_CONDITIONS)
    

#-----------------------------------------------------------------------------------------------------------------------
# Local rules are rules that won't be submitted to the scheduler but executed in the current session (front-end or node)
#-----------------------------------------------------------------------------------------------------------------------
localrules: get_fastq_se, get_fastq_pe, filter_peaks_macs, filter_peaks_genrich, filter_peaks_genrich_merge, peaks, bams, all, all_server


#-----------------------------------------------------------------------------------------------------------------------
# Define multiple outputs based on the output files desired
#-----------------------------------------------------------------------------------------------------------------------

# rule all:
#     input: ALL_PEAKANNOT

rule peaks:
    input: ALL_PEAKS

rule bams:
    input: ALL_BAMs #+ ALL_BIGWIG 

rule all_server:
    input: ALL_BW2SERVER

# rule only_idr:
#     input: ALL_IDR

# ----- Load rules in external files ----- #
include: "rules/functions.smk"
include: "rules/trimming.smk"
include: "rules/align.smk"
include: "rules/bam2bw.smk"
include: "rules/macs2.smk"
include: "rules/genrich.smk"
include: "rules/QC.smk"
include: "rules/idr.smk"
# include: "rules/prepare2GEO.smk

# ----- handle possible errors, clean temp folders ----- #
# Remove the folder used to create the fastq files (snakemake removes the tmp files but not the folder...)
# Since some jobs a lot of times end in E state after finishing (when they're too fast, like creating a soft link),
# remove those "canceled" jobs after the pipeline ends
onsuccess:
    shell("""
    rm -r fastq/
    qselect -u `whoami` -s E | xargs qdel -Wforce
    """)

onerror:
    print("An error ocurred. Workflow aborted")
    shell("""
        mail -s "An error occurred. ATAC-seq snakemake workflow aborted" `whoami`@ieo.it < {log}
        """)

# END 
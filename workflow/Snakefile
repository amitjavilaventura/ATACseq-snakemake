# ----------------------------------------------------------------------------------------------- #
# SNAKEFILE
# ----------------------------------------------------------------------------------------------- #

# ----- Import libraries ----- #
import pandas as pd
import yaml
from snakemake.utils import validate, min_version

# ----- Minimum Snakemake version ----- #
min_version("5.4.3")

# ----- Singularity image path ----- #
singularity: "/hpcnfs/data/DP/Singularity/atacseq_snakemake_latest.sif"

# ----- Configfile ----- #
configfile: "configuration/config.yaml"

# ----- CLUSTER FILE ----- #
# variable for the cluster file 
CLUSTER     = yaml.load(open(config['cluster'], 'r'), Loader=yaml.FullLoader)

# Determine files for samples and units
SAMPLES     = pd.read_csv(config['samples'], sep = "\t").set_index("NAME", drop=False).sort_index()
units       = pd.read_csv(config['units'], dtype=str, sep = "\t").set_index(["sample", "lane"], drop=False).sort_index()
units.index = units.index.set_levels([i.astype(str) for i in units.index.levels])  # enforce str in index

# ----- Get the samples name, input, etc ----- #
ALL_SAMPLES    = SAMPLES.NAME
ALL_CONTROLS   = SAMPLES.INPUT
SAMPLES_GENOME = SAMPLES.GENOME

ALL_CONDITIONS = set(SAMPLES.CONDITION)


# ----- Determine output files ----- #
ALL_BAMs         = expand("results/02_bam/{sample}.bam", sample=ALL_SAMPLES)


ALL_FASTQCs      = expand("results/01_QCs/fastQC/{sample}_fastqc.zip", sample=ALL_SAMPLES)
ALL_FINGERPRINT  = expand("results/01_QCs/fingerPrint/{sample}.plot.pdf", sample = ALL_SAMPLES)
ALL_GCBIAS       = expand("results/01_QCs/GCbias/{sample}_GCbias.pdf", sample = ALL_SAMPLES)
MULTIQC          = ["results/01_QCs/multiQC/multiQC_report.html"]


ALL_BIGWIG       = expand("results/05_bigwig/noSubtract/{sample}.bw", sample=ALL_SAMPLES)
ALL_BW2SERVER    = expand("results/temp_file_{sample}.txt", sample=ALL_SAMPLES)


if config["options"]["genrich_pool"]:
    ALL_PEAKS        = expand("results/03_genrich/pooled/{condition}/{condition}_peaks.narrowPeak", zip, condition=ALL_CONDITIONS)
    ALL_PEAKS_FILT   = expand("results/03_genrich/pooled/{condition}/{condition}_peaks_" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt"] + ".bed", zip, condition=ALL_CONDITIONS)

    ALL_SUMMITS      = expand("results/03_genrich/pooled/{condition}/{condition}_summits.bed", zip, condition=ALL_CONDITIONS)
    ALL_SUMMITS_FILT = expand("results/03_genrich/pooled/{condition}/{condition}_summits_" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt"] + ".bed", zip, condition=ALL_CONDITIONS)
    
    ALL_PEAKANNOT    = expand("results/04_peakAnno/pooled/{condition}/{condition}-peaks_log" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt"] + ".annot", zip, condition=ALL_CONDITIONS)

elif config["options"]["genrich_pool"] == False:
    ALL_PEAKS        = expand("results/03_genrich/{sample}/{sample}_peaks.narrowPeak", zip, sample=ALL_SAMPLES)
    ALL_PEAKS_FILT   = expand("results/03_genrich/{sample}/{sample}_peaks_" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt"] + ".bed", zip, sample=ALL_SAMPLES)

    ALL_SUMMITS      = expand("results/03_genrich/{sample}/{sample}_summits.bed", zip, sample=ALL_SAMPLES)
    ALL_SUMMITS_FILT = expand("results/03_genrich/{sample}/{sample}_summits_" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt"] + ".bed", zip, sample=ALL_SAMPLES)

    ALL_PEAKANNOT    = expand("results/04_peakAnno/{sample}/{sample}-peaks_log" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt"] + ".annot", zip, sample=ALL_SAMPLES)


#-----------------------------------------------------------------------------------------------------------------------
# Local rules are rules that won't be submitted to the scheduler but executed in the current session (front-end or node)
#-----------------------------------------------------------------------------------------------------------------------
localrules: get_fastq_pe, filter_genrich, summits_genrich, all, all_server, get_peaks, get_peak_anno, get_summits, get_bams, get_bw, get_multiqc, get_fastqc, get_fingerprint


#-----------------------------------------------------------------------------------------------------------------------
# Define multiple outputs based on the output files desired
#-----------------------------------------------------------------------------------------------------------------------

rule all:
    input: ALL_PEAKANNOT + ALL_SUMMITS + MULTIQC + ALL_BIGWIG 

rule all_server:
    input: ALL_BW2SERVER

# peak rules
rule get_peaks:
    input: ALL_PEAKS + ALL_PEAKS_FILT

rule get_peakanno:
    input: ALL_PEAKANNOT

rule get_summits:
    input: ALL_SUMMITS + ALL_SUMMITS_FILT

# bam bw rules
rule get_bams:
    input: ALL_BAMs #+ ALL_BIGWIG 

rule get_bw:
    input: ALL_BIGWIG 

# qc_rules
rule get_multiqc:
    input: MULTIQC

rule get_fastqc:
    input: ALL_FASTQCs

rule get_fingerprint:
    input: ALL_FINGERPRINT

# ----- Load rules in external files ----- #
include: "rules/functions.smk"
include: "rules/trimming.smk"
include: "rules/align.smk"
include: "rules/bam2bw.smk"
include: "rules/genrich2.smk"
include: "rules/QC.smk"
# include: "rules/prepare2GEO.smk

# ----- handle possible errors, clean temp folders ----- #
# Remove the folder used to create the fastq files (snakemake removes the tmp files but not the folder...)
# Since some jobs a lot of times end in E state after finishing (when they're too fast, like creating a soft link),
# remove those "canceled" jobs after the pipeline ends
onsuccess:
    shell("""
    rm -r fastq/
    qselect -u `whoami` -s E | xargs qdel -Wforce
    """)

onerror:
    print("An error ocurred. Workflow aborted")
    shell("""
        mail -s "An error occurred. ATAC-seq snakemake workflow aborted" `whoami`@ieo.it < {log}
        """)

# END 
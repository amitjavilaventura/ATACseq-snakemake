# ----- Import libraries ----- #
import pandas as pd
import yaml
from snakemake.utils import validate, min_version

# ----- Minimum Snakemake version ----- #
min_version("5.4.3")

# ----- Singularity image path ----- #
singularity: "/hpcnfs/data/DP/Singularity/atacseq_snakemake_latest.sif"

# ----- Configfile ----- #
configfile: "configuration/config.yaml"

# ----- CLUSTER FILE ----- #
# variable for the cluster file 
CLUSTER     = yaml.load(open(config['cluster'], 'r'), Loader=yaml.FullLoader)

# Determine files for samples and units
SAMPLES     = pd.read_csv(config['samples'], sep = "\t").set_index("NAME", drop=False).sort_index()
units       = pd.read_csv(config['units'], dtype=str, sep = "\t").set_index(["sample", "lane"], drop=False).sort_index()
units.index = units.index.set_levels([i.astype(str) for i in units.index.levels])  # enforce str in index

# ----- Get the samples name, input, etc ----- #
ALL_SAMPLES    = SAMPLES.NAME
ALL_CONTROLS   = SAMPLES.INPUT
SAMPLES_GENOME = SAMPLES.GENOME

ALL_CONDITIONS = set(SAMPLES.CONDITION)


# ----- Determine output files ----- #
ALL_BAMs         = expand("results/02_bam/{sample}.bam", sample=ALL_SAMPLES)


ALL_FASTQCs      = expand("results/01_QCs/fastQC/{sample}_fastqc.zip", sample=ALL_SAMPLES)
ALL_FINGERPRINT  = expand("results/01_QCs/fingerPrint/{sample}.plot.pdf", sample = ALL_SAMPLES)
ALL_GCBIAS       = expand("results/01_QCs/GCbias/{sample}_GCbias.pdf", sample = ALL_SAMPLES)
MULTIQC          = ["results/01_QCs/multiQC/multiQC_report.html"]



ALL_BIGWIG       = expand("results/02_bigwig/{sample}.bw", sample=ALL_SAMPLES)
ALL_BW2SERVER    = expand("results/temp_file_{sample}.txt", sample=ALL_SAMPLES)


if config["options"]["genrich_merge"] == False:
    ALL_PEAKS      = expand("results/03_genrich/{sample}/{sample}_peaks.narrowPeak", zip, sample=ALL_SAMPLES)
    ALL_PEAKS_FILT = expand("results/03_genrich/{sample}/{sample}_peaks_" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt_peaks_pqval"] + ".bed", zip, sample=ALL_SAMPLES)
    ALL_PEAKANNOT  = expand("results/04_peakAnno/{sample}/{sample}-peaks_log" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt_peaks_pqval"] + ".annot", zip, sample=ALL_SAMPLES)


elif config["options"]["genrich_merge"] == True:
    ALL_PEAKS      = expand("results/03_genrich/merged/{condition}/{condition}_peaks.narrowPeak", zip, condition=ALL_CONDITIONS)
    ALL_PEAKS_FILT = expand("results/03_genrich/merged/{condition}/{condition}_peaks_" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt_peaks_pqval"] + ".bed", zip, condition=ALL_CONDITIONS)
    ALL_PEAKANNOT  = expand("results/04_peakAnno/merged/{condition}/{condition}-peaks_log" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt_peaks_pqval"] + ".annot", zip, condition=ALL_CONDITIONS)


#-----------------------------------------------------------------------------------------------------------------------
# Local rules are rules that won't be submitted to the scheduler but executed in the current session (front-end or node)
#-----------------------------------------------------------------------------------------------------------------------
localrules: get_fastq_se, get_fastq_pe, filter_peaks_genrich, filter_peaks_genrich_merge, peaks, bams, all, all_server


#-----------------------------------------------------------------------------------------------------------------------
# Define multiple outputs based on the output files desired
#-----------------------------------------------------------------------------------------------------------------------

rule all:
    input: ALL_PEAKANNOT + ALL_QCs #+ ALL_BIGWIG 

rule all_server:
    input: ALL_BW2SERVER

# peak rules
rule get_peaks:
    input: ALL_PEAKS + ALL_PEAKS_FILT

rule get_peak_anno:
    input: ALL_PEAKANNOT

# bam bw rules
rule get_bams:
    input: ALL_BAMs #+ ALL_BIGWIG 

rule get_bigwigs:
    input: ALL_BIGWIG 

rule bams_qc:
    input: ALL_BAMs + MULTIQC 

# qc_rules
rule get_qc:
    input: MULTIQC

rule get_fastqc:
    input: ALL_FASTQCs

rule get_fingerprint:
    input: ALL_FINGERPRINT

rule get_gcbias:
    input: ALL_GCBIAS

# ----- Load rules in external files ----- #
include: "rules/functions.smk"
include: "rules/trimming.smk"
include: "rules/align.smk"
include: "rules/bam2bw.smk"
include: "rules/genrich.smk"
include: "rules/QC.smk"
# include: "rules/prepare2GEO.smk

# ----- handle possible errors, clean temp folders ----- #
# Remove the folder used to create the fastq files (snakemake removes the tmp files but not the folder...)
# Since some jobs a lot of times end in E state after finishing (when they're too fast, like creating a soft link),
# remove those "canceled" jobs after the pipeline ends
onsuccess:
    shell("""
    rm -r fastq/
    qselect -u `whoami` -s E | xargs qdel -Wforce
    """)

onerror:
    print("An error ocurred. Workflow aborted")
    shell("""
        mail -s "An error occurred. ATAC-seq snakemake workflow aborted" `whoami`@ieo.it < {log}
        """)

# END 
# ----- Import libraries ----- #
import pandas as pd
import yaml
from snakemake.utils import validate, min_version

# ----- Minimum Snakemake version ----- #
min_version("5.4.3")

# ----- Singularity image path ----- #
singularity: "/hpcnfs/data/DP/Singularity/dfernandezperez-bioinformatics-singularity-master-chipseq.simg"

# ----- Configfile ----- #
configfile: "configuration/config.yaml"

# ----- CLUSTER FILE ----- #
# variable for the cluster file 
CLUSTER     = yaml.load(open(config['cluster'], 'r'), Loader=yaml.FullLoader)

# Determine files for samples and units
SAMPLES     = pd.read_csv(config['samples'], sep = "\t").set_index("NAME", drop=False).sort_index()
units       = pd.read_csv(config['units'], dtype=str, sep = "\t").set_index(["sample", "lane"], drop=False).sort_index()
units.index = units.index.set_levels([i.astype(str) for i in units.index.levels])  # enforce str in index

# ----- Get the samples name, input, etc ----- #
ALL_SAMPLES  = SAMPLES.NAME
ALL_CONTROLS = SAMPLES.INPUT
SAMPLES_GENOME  = SAMPLES.GENOME


# ----- Determine output files ----- #
ALL_BAMs         = expand("results/02_bam/{sample}.bam", zip, sample = ALL_SAMPLES)

ALL_QCs          = ["results/01_QCs/multiQC/multiQC_report.html"]
ALL_FASTQCs      = expand("results/01_QCs/fastQC/{sample}_fastqc.zip", zip, sample=ALL_SAMPLES)

ALL_BIGWIG       = expand("results/02_bigwig/{sample}.bw", zip, sample=ALL_SAMPLES)
ALL_BIGWIG_noSub = expand("results/02_bigwig/noSubtract/{sample}.bw", zip, sample=ALL_SAMPLES)

ALL_BW2SERVER    = expand("results/temp_file_{sample}_{control}.txt",  zip, sample = ALL_SAMPLES, control = ALL_CONTROLS)

if config["options"]["peakcaller"] == "macs":
    ALL_PEAKS      = expand("results/03_macs2/{sample}/{sample}_peaks.narrowPeak", zip, sample=ALL_SAMPLES)
    ALL_PEAKANNOT  = expand("results/04_peakAnno/{sample}/{sample}-peaks_logp" + config["params"]["macs2"]["filt_peaks_pval"] + ".annot", zip, sample=ALL_SAMPLES)
    
elif config["options"]["peakcaller"] == "genrich":
    ALL_PEAKS      = expand("results/03_genrich/{sample}/{sample}_peaks.narrowPeak", zip, sample=ALL_SAMPLES)
    ALL_PEAKANNOT  = expand("results/04_peakAnno/{sample}/{sample}-peaks_log" + config["params"]["genrich"]["p_or_q"] + config["params"]["genrich"]["filt_peaks_pqval"] + ".annot", zip, sample=ALL_SAMPLES)
    

###########################
#          RULES          #
###########################


rule all:
    input: ALL_PEAKS + ALL_PEAKANNOT + ALL_FASTQCs + ALL_QCs #+ ALL_BIGWIG_noSub #+ ALL_BIGWIG

rule peaks:
    input: ALL_PEAKS

rule bams:
    input: ALL_BAMs + ALL_BIGWIG_noSub #+ ALL_BIGWIG 

rule all_server:
    input: ALL_BW2SERVER

# ----- Load rules in external files ----- #
include: "rules/functions.smk"
include: "rules/trimming.smk"
include: "rules/align.smk"
include: "rules/bam2bw.smk"
include: "rules/peaks.smk"
include: "rules/QC.smk"
# include: "rules/idr.smk"
# include: "rules/prepare2GEO.smk

# ----- handle possible errors, clean temp folders ----- #
# Remove the folder used to create the fastq files (snakemake removes the tmp files but not the folder...)
# Since some jobs a lot of times end in E state after finishing (when they're too fast, like creating a soft link),
# remove those "canceled" jobs after the pipeline ends
onsuccess:
    shell("""
    rm -r fastq/
    qselect -u `whoami` -s E | xargs qdel
    """)

onerror:
    print("An error ocurred. Workflow aborted")
    shell("""
        mail -s "An error occurred. ATAC-seq snakemake workflow aborted" `whoami`@ieo.it < {log}
        """)

# END 